geom_point()+
xlab(TeX("\\textit{j}"))+
ylab(TeX("$|\\beta_j|$"))+
theme_classic()
rm(list = ls())
library(MASS)
library(tikzDevice)
library(latex2exp)
set.seed(123)
simulate_data <- function(n, p, kappa){
beta <- numeric(p)
sigma2 <- numeric(p)
# Generate beta coefficients
for (j in seq(p)){
a_j <- j^0.51
lambda_j <- ifelse(1-kappa*a_j>0,1-kappa*a_j,0)
sigma2_j <- lambda_j*(n*kappa*a_j)^(-1)
sigma2[j] <- sigma2_j
beta[j] <- rnorm(1, 0,sqrt(sigma2_j))
lambda_j <- NULL
}
# True data generating process
mu_vector <- numeric(p)
covariance_matrix <- diag(p)
X_design <- mvrnorm(n, mu = mu_vector, Sigma = covariance_matrix)
f_true <- numeric(n)
for (j in seq(p)){
f_true <- f_true + beta[j]*X_design[,j]
}
return(beta)
}
n <- 100
p <- 23
kappa <- 0.199
# calculation by wolfram alpha sum j^0.51*max(1-0.15*j^0.51,0), j=1 to infinity
k_test <- 0.15
conv_ser <- 45.7982
n_new <- conv_ser/k_test
p_new <- exp(-log(k_test)*(1/0.51))
#
# k_test2 <- 0.12
# conv_ser2 <- 88.8431
# n_new2 <- conv_ser2/k_test2
# p_new2 <- exp(-log(k_test2)*(1/0.51))
# Train and Test Data
train_data <- simulate_data(n=n, p=p, kappa = kappa)
train_data1 <- simulate_data(n=ceiling(n_new), p=ceiling(p_new), kappa = k_test)
library(ggplot2)
library(gridExtra)
install.packages("gridExtra")
library(gridExtra)
pl1 <- ggplot(data.frame(x=seq(abs(train_data)),y=abs(train_data)), aes(y=y, x=x)) +
geom_point()+
xlab(TeX("\\textit{j}"))+
ylab(TeX("$|\\beta_j|$"))+
theme_classic()
install.packages("latex2exp")
rm(list = ls())
library(MASS)
library(tikzDevice)
library(latex2exp)
set.seed(123)
simulate_data <- function(n, p, kappa){
beta <- numeric(p)
sigma2 <- numeric(p)
# Generate beta coefficients
for (j in seq(p)){
a_j <- j^0.51
lambda_j <- ifelse(1-kappa*a_j>0,1-kappa*a_j,0)
sigma2_j <- lambda_j*(n*kappa*a_j)^(-1)
sigma2[j] <- sigma2_j
beta[j] <- rnorm(1, 0,sqrt(sigma2_j))
lambda_j <- NULL
}
# True data generating process
mu_vector <- numeric(p)
covariance_matrix <- diag(p)
X_design <- mvrnorm(n, mu = mu_vector, Sigma = covariance_matrix)
f_true <- numeric(n)
for (j in seq(p)){
f_true <- f_true + beta[j]*X_design[,j]
}
return(beta)
}
n <- 100
p <- 23
kappa <- 0.199
# calculation by wolfram alpha sum j^0.51*max(1-0.15*j^0.51,0), j=1 to infinity
k_test <- 0.15
conv_ser <- 45.7982
n_new <- conv_ser/k_test
p_new <- exp(-log(k_test)*(1/0.51))
#
# k_test2 <- 0.12
# conv_ser2 <- 88.8431
# n_new2 <- conv_ser2/k_test2
# p_new2 <- exp(-log(k_test2)*(1/0.51))
# Train and Test Data
train_data <- simulate_data(n=n, p=p, kappa = kappa)
train_data1 <- simulate_data(n=ceiling(n_new), p=ceiling(p_new), kappa = k_test)
# train_data2 <- simulate_data(n=ceiling(n_new2), p=ceiling(p_new2), kappa = k_test2)
library(ggplot2)
library(gridExtra)
pl1 <- ggplot(data.frame(x=seq(abs(train_data)),y=abs(train_data)), aes(y=y, x=x)) +
geom_point()+
xlab(TeX("\\textit{j}"))+
ylab(TeX("$|\\beta_j|$"))+
theme_classic()
pl1
library(ggplot2)
library(gridExtra)
pl1 <- ggplot(data.frame(x=seq(abs(train_data)),y=abs(train_data)), aes(y=y, x=x)) +
geom_point()+
xlab(TeX("\\textit{j}"))+
ylab(TeX("$|\\beta_j|$"))+
theme_classic()
pl2 <- ggplot(data.frame(x=seq(abs(train_data1)),y=abs(train_data1)), aes(y=y, x=x)) +
geom_point()+
xlab(TeX("\\textit{j}"))+
ylab(TeX(""))+
theme_classic()
pdf('../../06_Plots/Main_Simulation/44_beta_coefficients.pdf', width = 6, height=4)
grid.arrange(pl1, pl2, ncol = 2)
dev.off()
library(dplyr)
library(mboost)
install.packages("mboost")
library(mboost)
install.packages("mboost")
library(mboost)
glmboost()
install.packages("libcoin")
update.packages()
install.packages("mboost")
library(mboost)
install.packages("libcoin")
install.packages("libcoin")
library(mboost)
glmboost()
library(mboost)
e <- rnorm(100,0,1)
x <- rnorm{100,0,1}
y <- 3*x + e
glmboost(y ~x)
e <- rnorm(100,0,1)
x <- rnorm(100,0,1)
y <- 3*x + e
glmboost(y ~x)
gamboost(y ~x)
installed.packages()
max(y)
rm(list = ls())
library(MASS)
library(tikzDevice)
library(latex2exp)
set.seed(123)
simulate_data <- function(n, p, kappa){
beta <- numeric(p)
sigma2 <- numeric(p)
# Generate beta coefficients
for (j in seq(p)){
a_j <- j^0.51
lambda_j <- ifelse(1-kappa*a_j>0,1-kappa*a_j,0)
sigma2_j <- lambda_j*(n*kappa*a_j)^(-1)
sigma2[j] <- sigma2_j
beta[j] <- rnorm(1, 0,sqrt(sigma2_j))
lambda_j <- NULL
}
# True data generating process
mu_vector <- numeric(p)
covariance_matrix <- diag(p)
X_design <- mvrnorm(n, mu = mu_vector, Sigma = covariance_matrix)
f_true <- numeric(n)
for (j in seq(p)){
f_true <- f_true + beta[j]*X_design[,j]
}
return(beta)
}
n <- 100
p <- 23
kappa <- 0.199
# calculation by wolfram alpha sum j^0.51*max(1-0.15*j^0.51,0), j=1 to infinity
k_test <- 0.15
conv_ser <- 45.7982
n_new <- conv_ser/k_test
p_new <- exp(-log(k_test)*(1/0.51))
#
# k_test2 <- 0.12
# conv_ser2 <- 88.8431
# n_new2 <- conv_ser2/k_test2
# p_new2 <- exp(-log(k_test2)*(1/0.51))
# Train and Test Data
train_data <- simulate_data(n=n, p=p, kappa = kappa)
train_data1 <- simulate_data(n=ceiling(n_new), p=ceiling(p_new), kappa = k_test)
# train_data2 <- simulate_data(n=ceiling(n_new2), p=ceiling(p_new2), kappa = k_test2)
library(ggplot2)
library(gridExtra)
pl1 <- ggplot(data.frame(x=seq(abs(train_data)),y=abs(train_data)), aes(y=y, x=x)) +
geom_point()+
xlab(TeX("\\textit{j}"))+
ylab(TeX("$|\\beta_j|$"))+
ylim(0, max(y))
theme_classic()
pl2 <- ggplot(data.frame(x=seq(abs(train_data1)),y=abs(train_data1)), aes(y=y, x=x)) +
geom_point()+
xlab(TeX("\\textit{j}"))+
ylab(TeX(""))+
ylim(0, max(abs(train_data)))
theme_classic()
pl2
pl1
pl1 <- ggplot(data.frame(x=seq(abs(train_data)),y=abs(train_data)), aes(y=y, x=x)) +
geom_point()+
xlab(TeX("\\textit{j}"))+
ylab(TeX("$|\\beta_j|$"))+
ylim(0, max(y))+
theme_classic()
pl1 <- ggplot(data.frame(x=seq(abs(train_data)),y=abs(train_data)), aes(y=y, x=x)) +
geom_point()+
xlab(TeX("\\textit{j}"))+
ylab(TeX("$|\\beta_j|$"))+
ylim(0, max(abs(train_data)))+
theme_classic()
pl1
pl2
pl2 <- ggplot(data.frame(x=seq(abs(train_data1)),y=abs(train_data1)), aes(y=y, x=x)) +
geom_point()+
xlab(TeX("\\textit{j}"))+
ylab(TeX(""))+
ylim(0, max(abs(train_data)))+
theme_classic()
pl2
pl1
pl1 <- ggplot(data.frame(x=seq(abs(train_data)),y=abs(train_data)), aes(y=y, x=x)) +
geom_point()+
xlab(TeX("\\textit{j}"))+
ylab(TeX("$|\\beta_j|$"))+
ylim(0, max(abs(train_data)))+
theme_bw()+
theme(panel.grid = element_blank())
pl1
pl2
library(ggplot2)
library(gridExtra)
pl1 <- ggplot(data.frame(x=seq(abs(train_data)),y=abs(train_data)), aes(y=y, x=x)) +
geom_point()+
xlab(TeX("\\textit{j}"))+
ylab(TeX("$|\\beta_j|$"))+
ylim(0, max(abs(train_data)))+
theme_bw()+
theme(panel.grid = element_blank())
pl2 <- ggplot(data.frame(x=seq(abs(train_data1)),y=abs(train_data1)), aes(y=y, x=x)) +
geom_point()+
xlab(TeX("\\textit{j}"))+
ylab(TeX(""))+
ylim(0, max(abs(train_data)))+
theme_bw()+
theme(panel.grid = element_blank())
pdf('../../06_Plots/Main_Simulation/44_beta_coefficients.pdf', width = 6, height=4)
grid.arrange(pl1, pl2, ncol = 2)
dev.off()
rm(list =ls())
library(tikzDevice)
'
This script plots the bias-variance tradeoff and the MSE of the model (4.2) in Bühlmann/Yu
with varying numbers of boosting iterations.
Sources: Data is generated by the following scripts
42_boost_iter_bias_variance: bias/variance/mse of boosted model
'
n_boost <- 300 #250
boost_variance_bias_mse <- readRDS(file = "../99_Stored_Data/42_boost_iter_bias_variance.rds")
### MSE decomposition
tikz('../../06_Plots/Main_Simulation/42_boost_iter_mse_decomp.tex', width = 6, height=4)
plot(seq(n_boost), boost_variance_bias_mse[3,1:300],
type = "l",
ylim = c(0,5),
xlab = "Boosting iterations",
ylab = "",
main = "Uncorrelated Design - MSE Decomposition")
lines(seq(n_boost), boost_variance_bias_mse[1,1:300], lty = 2)
lines(seq(n_boost), boost_variance_bias_mse[2,1:300], lty = 3)
dev.off()
### comparison l2boost (development and aic) vs lasso
tikz('../../06_Plots/Main_Simulation/model42_boost_iter.tex', width = 6, height=4)
plot(seq(n_boost), boost_variance_bias_mse[3,1:300],
type = "l",
ylim = c(2,6),
xlab = "Boosting iterations",
ylab = "MSE",
main = "Uncorrelated Design")
abline(h=2.7864, lty = 2) #l2boost
abline(h=3.0026, lty = 3) #lasso
dev.off()
#### GGPLOT2 IMPLEMENTATION
library(ggplot2)
library(gridExtra)
df <- data.frame(x = seq(n_boost), MSE = boost_variance_bias_mse[3,1:300], variance = boost_variance_bias_mse[1,1:300], bias = boost_variance_bias_mse[2,1:300])
df_plot <- df %>%
ggplot()+
geom_line(aes(x=x, y= MSE, linetype = "MSE"))+
geom_line(aes(x=x, y= variance, linetype = "Variance"))+
geom_line(aes(x=x, y= bias, linetype = "Squared Bias"))+
xlab("Boosting iterations")+
ylim(0,5)+
ggtitle("Uncorrelated Design - MSE Decomposition")
theme_bw()+
theme(panel.grid = element_blank())+
scale_linetype_manual(name = "",
values=c(3,1,2), labels=c("MSE","Squared Bias", "Variance"))
library(dplyr)
theme_bw()+
theme(panel.grid = element_blank())+
scale_linetype_manual(name = "",
values=c(3,1,2), labels=c("MSE","Squared Bias", "Variance"))
df_plot <- df %>%
ggplot()+
geom_line(aes(x=x, y= MSE, linetype = "MSE"))+
geom_line(aes(x=x, y= variance, linetype = "Variance"))+
geom_line(aes(x=x, y= bias, linetype = "Squared Bias"))+
xlab("Boosting iterations")+
ylim(0,5)+
ggtitle("Uncorrelated Design - MSE Decomposition")+
theme_bw()+
theme(panel.grid = element_blank())+
scale_linetype_manual(name = "",
values=c(3,1,2), labels=c("MSE","Squared Bias", "Variance"))
df_plot
df_plot <- df %>%
ggplot()+
geom_line(aes(x=x, y= MSE, linetype = "MSE"))+
geom_line(aes(x=x, y= variance, linetype = "Variance"))+
geom_line(aes(x=x, y= bias, linetype = "Squared Bias"))+
xlab("Boosting iterations")+
ylim(0,5)+
ggtitle("Uncorrelated Design - MSE Decomposition")+
theme_bw()+
theme(panel.grid = element_blank())+
scale_linetype_manual(name = "",
values=c(1,3,2), labels=c("MSE","Squared Bias", "Variance"))
df_plot
df_plot <- df %>%
ggplot()+
geom_line(aes(x=x, y= MSE, linetype = "MSE"))+
geom_line(aes(x=x, y= variance, linetype = "Variance"))+
geom_line(aes(x=x, y= bias, linetype = "Squared Bias"))+
xlab("Boosting iterations")+
ylim(0,5)+
ggtitle("Uncorrelated Design - MSE Decomposition")+
theme_bw()+
theme(panel.grid = element_blank(), legend.position = "bottom")+
scale_linetype_manual(name = "",
values=c(1,3,2), labels=c("MSE","Squared Bias", "Variance"))
df_plot
ggsave(df_plot, file = '../../06_Plots/Main_Simulation/42_boost_iter_mse_decomp.pdf', width = 6, height = 4)
rm(list =ls())
'
This script plots the bias-variance tradeoff and the MSE of the model (4.3) in Bühlmann/Yu
with varying numbers of boosting iterations.
Sources: Data is generated by the following scripts
43_boost_iter_bias_variance: bias/variance of boosted model
'
n_boost <- 300 #250
boost_variance_bias_mse <- readRDS(file = "../99_Stored_Data/43_boost_iter_bias_variance.rds")
#### MSE decomposition
pdf('../../06_Plots/Main_Simulation/43_boost_iter_mse_decomp.pdf', width = 6, height = 4)
plot(seq(n_boost), boost_variance_bias_mse[3,1:300],
type = "l",
ylim = c(0,5),
xlab = "Boosting iterations",
ylab = "",
main = "Correlated Design - MSE Development")
dev.off()
### comparison l2boost (development & aic) vs lasso
png('../../06_Plots/Main_Simulation/model43_boost_iter.png', res=600, height=10, width=10, units="in")
plot(seq(n_boost), boost_variance_bias_mse[3,1:300],
type = "l",
ylim = c(1,6),
xlab = "Boosting iterations",
ylab = "MSE",
main = "Correlated Design")
abline(h=1.4853, lty = 2) #l2boost
abline(h=1.7796, lty = 3) #lasso
dev.off()
#### GGPLOT2 IMPLEMENTATION
library(ggplot2)
library(dplyr)
library(gridExtra)
df <- data.frame(x = seq(n_boost), MSE = boost_variance_bias_mse[3,1:300], variance = boost_variance_bias_mse[1,1:300], bias = boost_variance_bias_mse[2,1:300])
df_plot <- df %>%
ggplot()+
geom_line(aes(x=x, y= MSE, linetype = "MSE"))+
geom_line(aes(x=x, y= variance, linetype = "Variance"))+
geom_line(aes(x=x, y= bias, linetype = "Squared Bias"))+
xlab("Boosting iterations")+
ylim(0,5)+
ggtitle("Uncorrelated Design - MSE Decomposition")+
theme_bw()+
theme(panel.grid = element_blank(), legend.position = "bottom")+
scale_linetype_manual(name = "",
values=c(1,3,2), labels=c("MSE","Squared Bias", "Variance"))
ggsave(df_plot, file = '../../06_Plots/Main_Simulation/43_boost_iter_mse_decomp.pdf', width = 6, height = 4)
df_plot
df <- data.frame(x = seq(n_boost), MSE = boost_variance_bias_mse[3,1:300], variance = boost_variance_bias_mse[1,1:300]-0.1, bias = boost_variance_bias_mse[2,1:300])
df_plot <- df %>%
ggplot()+
geom_line(aes(x=x, y= MSE, linetype = "MSE"))+
geom_line(aes(x=x, y= variance, linetype = "Variance"))+
geom_line(aes(x=x, y= bias, linetype = "Squared Bias"))+
xlab("Boosting iterations")+
ylim(0,5)+
ggtitle("Uncorrelated Design - MSE Decomposition")+
theme_bw()+
theme(panel.grid = element_blank(), legend.position = "bottom")+
scale_linetype_manual(name = "",
values=c(1,3,2), labels=c("MSE","Squared Bias", "Variance"))
df_plot
ggsave(df_plot, file = '../../06_Plots/Main_Simulation/43_boost_iter_mse_decomp.pdf', width = 6, height = 4)
df <- data.frame(x = seq(150), MSE = boost_variance_bias_mse[3,1:150], variance = boost_variance_bias_mse[1,1:150], bias = boost_variance_bias_mse[2,1:150])
df_plot <- df %>%
ggplot()+
geom_line(aes(x=x, y= MSE, linetype = "MSE"))+
geom_line(aes(x=x, y= variance, linetype = "Variance"))+
geom_line(aes(x=x, y= bias, linetype = "Squared Bias"))+
xlab("Boosting iterations")+
ylim(0,5)+
ggtitle("Uncorrelated Design - MSE Decomposition")+
theme_bw()+
theme(panel.grid = element_blank(), legend.position = "bottom")+
scale_linetype_manual(name = "",
values=c(1,3,2), labels=c("MSE","Squared Bias", "Variance"))
df_plot
df_plot <- df %>%
ggplot()+
geom_line(aes(x=x, y= MSE))+
xlab("Boosting iterations")+
ylim(0,5)+
ggtitle("Correlated Design - MSE Development")+
theme_bw()+
theme(panel.grid = element_blank(), legend.position = "none")
df_plot
ggsave(df_plot, file = '../../06_Plots/Main_Simulation/43_boost_iter_mse_decomp.pdf', width = 6, height = 4)
rm(list = ls())
library(MASS)
library(tikzDevice)
library(latex2exp)
set.seed(123)
simulate_data <- function(n, p, kappa){
beta <- numeric(p)
sigma2 <- numeric(p)
# Generate beta coefficients
for (j in seq(p)){
a_j <- j^0.51
lambda_j <- ifelse(1-kappa*a_j>0,1-kappa*a_j,0)
sigma2_j <- lambda_j*(n*kappa*a_j)^(-1)
sigma2[j] <- sigma2_j
beta[j] <- rnorm(1, 0,sqrt(sigma2_j))
lambda_j <- NULL
}
# True data generating process
mu_vector <- numeric(p)
covariance_matrix <- diag(p)
X_design <- mvrnorm(n, mu = mu_vector, Sigma = covariance_matrix)
f_true <- numeric(n)
for (j in seq(p)){
f_true <- f_true + beta[j]*X_design[,j]
}
return(beta)
}
n <- 100
p <- 23
kappa <- 0.199
# calculation by wolfram alpha sum j^0.51*max(1-0.15*j^0.51,0), j=1 to infinity
k_test <- 0.15
conv_ser <- 45.7982
n_new <- conv_ser/k_test
p_new <- exp(-log(k_test)*(1/0.51))
#
# k_test2 <- 0.12
# conv_ser2 <- 88.8431
# n_new2 <- conv_ser2/k_test2
# p_new2 <- exp(-log(k_test2)*(1/0.51))
# Train and Test Data
train_data <- simulate_data(n=n, p=p, kappa = kappa)
train_data1 <- simulate_data(n=ceiling(n_new), p=ceiling(p_new), kappa = k_test)
# train_data2 <- simulate_data(n=ceiling(n_new2), p=ceiling(p_new2), kappa = k_test2)
# tikz('../../06_Plots/Main_Simulation/44_beta_coefficients.tex', width = 6, height=4)
pdf('../../06_Plots/Main_Simulation/44_beta_coefficients.pdf', width = 6, height=4)
par(mfrow = c(1,2))
plot(abs(train_data), xlab = TeX("\\textit{j}"), ylab = TeX("$|\\beta_j|$"))
plot(abs(train_data1), xlab = TeX("\\textit{j}"), ylab = "" , ylim = c(0,max(abs(train_data))))#, yaxt='n')
# plot(abs(train_data2))
dev.off()
### GGPLOT Implementation ------
library(ggplot2)
library(gridExtra)
pl1 <- ggplot(data.frame(x=seq(abs(train_data)),y=abs(train_data)), aes(y=y, x=x)) +
geom_point()+
xlab(TeX("\\textit{j}"))+
ylab(TeX("$|\\beta_j|$"))+
ylim(0, max(abs(train_data)))+
theme_bw()+
theme(panel.grid = element_blank())
pl2 <- ggplot(data.frame(x=seq(abs(train_data1)),y=abs(train_data1)), aes(y=y, x=x)) +
geom_point()+
xlab(TeX("\\textit{j}"))+
ylab(TeX(""))+
ylim(0, max(abs(train_data)))+
theme_bw()+
theme(panel.grid = element_blank())
pdf('../../06_Plots/Main_Simulation/44_beta_coefficients.pdf', width = 6, height=3)
grid.arrange(pl1, pl2, ncol = 2)
dev.off()
